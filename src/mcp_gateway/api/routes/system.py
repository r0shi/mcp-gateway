import logging
from datetime import datetime, timedelta, timezone
from typing import Any

from fastapi import APIRouter, Depends
from sqlalchemy import delete, func, select, text
from sqlalchemy.ext.asyncio import AsyncSession

from mcp_gateway.api.deps import Principal, require_admin
from mcp_gateway.audit import log_audit
from mcp_gateway.db import get_session
from mcp_gateway.models import User
from mcp_gateway.minio_client import get_minio_client
from mcp_gateway.models import Chunk, Document, DocumentPage, DocumentVersion, IngestionJob
from mcp_gateway.models.enums import JobStage, JobStatus
from mcp_gateway.redis import get_async_redis

logger = logging.getLogger(__name__)
router = APIRouter(tags=["system"])


@router.get("/system/setup-status")
async def setup_status(
    session: AsyncSession = Depends(get_session),
) -> dict[str, bool]:
    """Check whether initial setup is needed (no users exist)."""
    count = await session.scalar(select(func.count()).select_from(User))
    return {"needs_setup": count == 0}


@router.get("/system/health")
async def health_check(
    session: AsyncSession = Depends(get_session),
) -> dict[str, Any]:
    """Check connectivity to Postgres, Redis, and MinIO."""
    checks: dict[str, Any] = {}

    # PostgreSQL
    try:
        result = await session.execute(text("SELECT 1"))
        result.scalar()
        checks["postgres"] = "ok"
    except Exception as e:
        logger.error("Postgres health check failed: %s", e)
        checks["postgres"] = f"error: {e}"

    # Redis
    try:
        redis = get_async_redis()
        await redis.ping()
        await redis.aclose()
        checks["redis"] = "ok"
    except Exception as e:
        logger.error("Redis health check failed: %s", e)
        checks["redis"] = f"error: {e}"

    # MinIO
    try:
        client = get_minio_client()
        client.bucket_exists("originals")
        checks["minio"] = "ok"
    except Exception as e:
        logger.error("MinIO health check failed: %s", e)
        checks["minio"] = f"error: {e}"

    overall = all(v == "ok" for v in checks.values())
    return {
        "status": "healthy" if overall else "degraded",
        "checks": checks,
    }


@router.get("/system/stats")
async def system_stats(
    admin: Principal = Depends(require_admin),
    session: AsyncSession = Depends(get_session),
) -> dict[str, Any]:
    """Return per-service performance/health stats (admin only)."""
    from redis import Redis
    from rq import Queue

    from mcp_gateway.config import get_settings

    settings = get_settings()
    result: dict[str, Any] = {}

    # ── PostgreSQL stats ──
    try:
        row = await session.execute(text(
            "SELECT pg_database_size(current_database()) AS db_size"
        ))
        db_size = row.scalar() or 0

        row = await session.execute(text(
            "SELECT count(*) FROM pg_stat_activity WHERE state IS NOT NULL"
        ))
        active_conns = row.scalar() or 0

        row = await session.execute(text(
            "SELECT sum(heap_blks_hit)::float "
            "/ nullif(sum(heap_blks_hit + heap_blks_read), 0) "
            "FROM pg_statio_user_tables"
        ))
        cache_hit = row.scalar()

        row = await session.execute(text("SELECT count(*) FROM chunks"))
        total_chunks = row.scalar() or 0

        row = await session.execute(text(
            "SELECT coalesce(sum(n_dead_tup), 0) FROM pg_stat_user_tables"
        ))
        dead_tuples = row.scalar() or 0

        result["postgres"] = {
            "db_size_mb": round(db_size / (1024 * 1024), 1),
            "active_connections": int(active_conns),
            "cache_hit_ratio": round(float(cache_hit), 4) if cache_hit is not None else None,
            "total_chunks": int(total_chunks),
            "dead_tuples": int(dead_tuples),
        }
    except Exception as e:
        logger.error("Failed to collect Postgres stats: %s", e)
        result["postgres"] = {"error": str(e)}

    # ── Redis stats ──
    try:
        redis = get_async_redis()
        info_mem = await redis.info("memory")
        info_clients = await redis.info("clients")
        await redis.aclose()

        # RQ queue depths (sync client needed)
        conn = Redis.from_url(settings.redis_url)
        io_depth = Queue("io", connection=conn).count
        cpu_depth = Queue("cpu", connection=conn).count
        conn.close()

        result["redis"] = {
            "used_memory_mb": round(info_mem.get("used_memory", 0) / (1024 * 1024), 1),
            "io_queue_depth": io_depth,
            "cpu_queue_depth": cpu_depth,
            "connected_clients": info_clients.get("connected_clients", 0),
        }
    except Exception as e:
        logger.error("Failed to collect Redis stats: %s", e)
        result["redis"] = {"error": str(e)}

    # ── MinIO stats ──
    try:
        client = get_minio_client()
        obj_count = 0
        total_size = 0
        for obj in client.list_objects("originals", recursive=True):
            obj_count += 1
            total_size += obj.size or 0
            if obj_count >= 10_000:
                break
        result["minio"] = {
            "object_count": obj_count,
            "total_size_mb": round(total_size / (1024 * 1024), 1),
        }
    except Exception as e:
        logger.error("Failed to collect MinIO stats: %s", e)
        result["minio"] = {"error": str(e)}

    return result


@router.post("/system/purge-run")
async def purge_run(
    admin: Principal = Depends(require_admin),
    session: AsyncSession = Depends(get_session),
) -> dict[str, Any]:
    """Hard-delete documents soft-deleted more than 60 days ago, including MinIO objects."""
    cutoff = datetime.now(timezone.utc) - timedelta(days=60)

    result = await session.execute(
        select(Document).where(
            Document.status == "deleted",
            Document.updated_at < cutoff,
        )
    )
    docs = result.scalars().all()

    if not docs:
        return {"purged": 0}

    minio = get_minio_client()
    purged = 0

    for doc in docs:
        # Load versions for MinIO cleanup
        versions_result = await session.execute(
            select(DocumentVersion).where(DocumentVersion.doc_id == doc.doc_id)
        )
        versions = versions_result.scalars().all()

        for ver in versions:
            # Delete MinIO object
            try:
                minio.remove_object(ver.original_bucket, ver.original_object_key)
            except Exception as e:
                logger.warning(
                    "Failed to delete MinIO object %s/%s: %s",
                    ver.original_bucket, ver.original_object_key, e,
                )

            # Cascade delete DB rows: chunks, pages, ingestion_jobs
            await session.execute(
                delete(Chunk).where(Chunk.version_id == ver.version_id)
            )
            await session.execute(
                delete(DocumentPage).where(DocumentPage.version_id == ver.version_id)
            )
            await session.execute(
                delete(IngestionJob).where(IngestionJob.version_id == ver.version_id)
            )

        # Delete versions and document
        await session.execute(
            delete(DocumentVersion).where(DocumentVersion.doc_id == doc.doc_id)
        )
        await session.delete(doc)
        purged += 1

    await log_audit(
        session, user_id=admin.id, action="purge_run",
        detail={"purged_count": purged},
    )
    await session.commit()

    logger.info("Purged %d documents", purged)
    return {"purged": purged}


@router.post("/system/reaper-run")
async def reaper_run(
    admin: Principal = Depends(require_admin),
    session: AsyncSession = Depends(get_session),
) -> dict[str, Any]:
    """Find ingestion jobs marked running in DB but absent from RQ, reset and re-enqueue."""
    from redis import Redis
    from rq import Queue
    from rq.job import Job as RQJob

    from mcp_gateway.config import get_settings
    from mcp_gateway.worker.pipeline import enqueue_stage

    settings = get_settings()
    conn = Redis.from_url(settings.redis_url)

    # Get all currently running jobs from DB
    result = await session.execute(
        select(IngestionJob).where(IngestionJob.status == JobStatus.running)
    )
    running_jobs = result.scalars().all()

    if not running_jobs:
        return {"reaped": 0}

    # Check which RQ queues have these jobs
    io_queue = Queue("io", connection=conn)
    cpu_queue = Queue("cpu", connection=conn)

    # Build set of all known RQ job IDs (started + queued registries)
    rq_job_ids: set[str] = set()
    for q in [io_queue, cpu_queue]:
        rq_job_ids.update(q.started_job_registry.get_job_ids())
        rq_job_ids.update(q.get_job_ids())

    reaped = 0
    for job in running_jobs:
        # Check if any RQ job references this version_id + stage
        # Since we don't store RQ job ID in our DB, check by scanning.
        # A simpler heuristic: if the job has been "running" for > 2x its timeout, reap it.
        from mcp_gateway.worker.pipeline import STAGE_CONFIG
        _, timeout, _ = STAGE_CONFIG[job.stage]
        if job.started_at is None:
            continue
        elapsed = (datetime.now(timezone.utc) - job.started_at).total_seconds()
        if elapsed < timeout * 2:
            continue

        # Orphan detected — re-enqueue
        logger.warning(
            "Reaping orphan job: version=%s stage=%s (running for %.0fs, timeout=%ds)",
            job.version_id, job.stage.value, elapsed, timeout,
        )
        await session.commit()  # flush before sync enqueue_stage
        enqueue_stage(job.version_id, job.stage)
        reaped += 1

    await log_audit(
        session, user_id=admin.id, action="reaper_run",
        detail={"reaped_count": reaped},
    )
    await session.commit()

    logger.info("Reaped %d orphan jobs", reaped)
    return {"reaped": reaped}
