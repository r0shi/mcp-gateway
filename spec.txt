Prototype Spec: Local Knowledge Appliance (Mac mini/Studio)
A) Goals
Web-first document dropbox for non-technical offices
Local extraction + OCR (English/French)
Local indexing: lexical + semantic + hybrid retrieval
Multi-user: admin and user
Cloud LLM can query via MCP over HTTPS using API keys (read-only)
Preserve privacy by not uploading the full corpus; return cited snippets only
Single-tenant appliance (no multi-tenancy)
B) Non-goals (prototype)
Fine-grained ACLs beyond role-based admin/user
Email ingestion/connectors
Auto-generated wiki narratives (future)
Multi-node cluster orchestration; no k8s
Built-in automated backup scheduling (documented ops only)
Multi-tenancy
C) Supported files
PDF (text layer extraction; OCR fallback)
DOCX
TXT/MD
RTF
JPEG (OCR "phone scan")
OCR languages:
English + French (tika unaffected; tesseract language packs required)
Upload limits (defaults, configurable):
Max file size: 200 MB
Max batch: 2 GB per session
Upload flow:
User uploads file(s) via web UI or API.
For each file, system computes SHA256.
If SHA256 matches an existing document version → status 'duplicate', return existing doc.
Otherwise, UI asks user: "New document" or "New version of <existing doc>?"
  (Future: auto-detect via content similarity; ask only when uncertain.)
If new document → create document family + first version.
If new version → create version under selected document family.
D) Architecture & Services (Docker Compose)
Containers:
gateway (Caddy) — HTTPS termination (self-signed CA), reverse proxy
app (FastAPI) — REST API + MCP endpoint + serves static UI
worker-io — RQ worker for io queue (extract/chunk/finalize)
worker-cpu — RQ worker for cpu queue (ocr/embed)
embedder — sentence-transformers model server (all-MiniLM-L6-v2, 384-dim)
postgres — Postgres + pgvector
redis — RQ queue
minio — object store for originals
tika — Apache Tika server (RTF and "weird office" fallback)
UI:
Vite + React + TypeScript SPA
Built to static files, served by FastAPI (mounted at /)
Networking:
External: HTTPS on 443 (80 redirects), self-signed CA via Caddy
Internal docker network: app talks to postgres/redis/minio/tika/embedder
Worker presets (configurable)
Let C = logical cores.
Quiet: io=1, cpu=1, max_concurrent_ocr=1
Balanced: io=max(2, C//4), cpu=1, max_concurrent_ocr=1
Fast: io=max(2, C//2), cpu=min(2, max(1, C//4)), max_concurrent_ocr=2
Hard caps (prototype):
io_workers <= 8
cpu_workers <= 2
E) Storage (MinIO + Postgres)
MinIO
Bucket:
originals
Keys:
originals/versions/<version_id>/<original_filename>
Postgres
Stores:
users, roles, api keys
documents, versions, uploads
ingestion job state + audit log
extracted page text (document_pages)
chunks (chunks) + FTS (bilingual) + embeddings
Deletion & purge:
Soft-delete doc families (documents.status='deleted')
Purge job permanently deletes DB rows + MinIO originals after 60 days (configurable)
Admin can "purge now"
Durability notes:
Persist volumes for postgres/minio
WAL enabled (default); no "fsync off" hacks in prototype
Provide backup/restore docs (pg_dump + minio bucket sync)
F) AuthN/AuthZ
Roles
Human users: admin / user
Bot: API key principal (read-only)
Human login
Email + password
JWT access token + refresh cookie (or session cookie; implement JWT+refresh)
API keys (for MCP / programmatic)
Admin creates keys (plaintext shown once)
Store only key_hash in DB
Header: Authorization: Bearer <api_key>
Keys are read-only for prototype (search/read/list/status)
G) Ingestion pipeline (stages + idempotence)
Stages:
extract (io)
ocr (cpu) — conditional
chunk (io)
embed (cpu) — calls embedder service over HTTP
finalize (io)
Idempotence:
Each stage is safe to rerun; stage runner checks DB state.
Concurrency guarded by row-level lock on (version_id, stage) in ingestion_jobs.
Job timeouts & failure handling:
Use RQ's native job_timeout per stage.
Wire on_failure callback that updates ingestion_jobs row to status='error'.
Stage enqueue logic checks ingestion_jobs state before proceeding to next stage.
Reaper job every 5 minutes handles orphaned jobs only:
  jobs marked 'running' in ingestion_jobs but absent from RQ registry → reset to 'queued' and re-enqueue.
Stage timeouts (defaults, set as RQ job_timeout):
extract: 10 min
ocr: 2 hours
chunk: 20 min
embed: 30 min
finalize: 10 min
OCR triggering
JPEG: always OCR (one "page")
PDF:
attempt text extraction
OCR if extracted_chars < 500 OR alpha_ratio < 0.2
DOCX/RTF/TXT: no OCR
Extraction tooling
PDF fast path: PyMuPDF
DOCX: python-docx
RTF: Apache Tika
Fallback for weird formats: Apache Tika
OCR tooling
Convert PDF pages to images via pdftoppm
Tesseract OCR with eng+fra (configurable)
Store per-page text and confidence proxy
Chunking (procedure-friendly)
Chunk by headings + paragraphs + lists (best-effort)
Target chunk length: ~1000 chars, overlap 150 chars (character-based in prototype)
Preserve mapping to page ranges + char offsets for citations
Detect language per chunk (for FTS column assignment)
Embeddings
Served by embedder container (all-MiniLM-L6-v2 or equivalent 384-dim model)
worker-cpu calls embedder over HTTP to get vectors
Store per chunk embedding in pgvector column
Similarity: cosine
H) Retrieval
Hybrid retrieval:
Lexical candidates via Postgres FTS (bilingual): top 30
  Query both fts_en and fts_fr columns; OR results
Vector candidates via pgvector cosine: top 30
Merge/dedupe; boost:
latest version in each doc family
higher OCR confidence
Return top K (default 10)
Conflict hint:
If top results include multiple versions/docs with similar scores → possible_conflict=true and include citations for competing sources.
Citations:
Always return citations with:
document title, filename
page_start/page_end
excerpt (short)
version timestamp
I) Postgres schema (DDL)
Extensions
CREATE EXTENSION IF NOT EXISTS pgcrypto;
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;
Users
CREATE TYPE user_role AS ENUM ('admin','user');

CREATE TABLE users (
  user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email CITEXT NOT NULL UNIQUE,
  password_hash TEXT NOT NULL,
  role user_role NOT NULL DEFAULT 'user',
  is_active BOOLEAN NOT NULL DEFAULT true,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  last_login_at TIMESTAMPTZ
);
API Keys
CREATE TABLE api_keys (
  key_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT NOT NULL,
  key_hash TEXT NOT NULL,
  is_active BOOLEAN NOT NULL DEFAULT true,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  last_used_at TIMESTAMPTZ
);
CREATE INDEX api_keys_active_idx ON api_keys(is_active);
Documents (families)
CREATE TABLE documents (
  doc_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  canonical_filename TEXT,
  latest_version_id UUID,
  status TEXT NOT NULL DEFAULT 'active', -- active|deleted
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX documents_updated_idx ON documents(updated_at DESC);
Uploads
CREATE TYPE upload_source AS ENUM ('web','watch_folder');

CREATE TABLE uploads (
  upload_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(user_id) ON DELETE SET NULL,
  source upload_source NOT NULL DEFAULT 'web',
  original_filename TEXT NOT NULL,
  mime_type TEXT,
  size_bytes BIGINT,
  sha256 BYTEA,
  minio_bucket TEXT NOT NULL,
  minio_object_key TEXT NOT NULL,
  doc_id UUID REFERENCES documents(doc_id) ON DELETE SET NULL,
  version_id UUID,
  status TEXT NOT NULL DEFAULT 'queued', -- queued|processing|done|error|duplicate
  error TEXT,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX uploads_created_idx ON uploads(created_at DESC);
Versions
CREATE TYPE version_status AS ENUM (
  'queued','extracting','extracted',
  'ocr_running','ocr_done',
  'chunking','chunked',
  'embedding','embedded',
  'ready','error'
);

CREATE TABLE document_versions (
  version_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  doc_id UUID NOT NULL REFERENCES documents(doc_id) ON DELETE CASCADE,
  original_sha256 BYTEA NOT NULL UNIQUE,
  original_bucket TEXT NOT NULL,
  original_object_key TEXT NOT NULL,
  mime_type TEXT,
  size_bytes BIGINT,
  status version_status NOT NULL DEFAULT 'queued',
  error TEXT,
  has_text_layer BOOLEAN,
  needs_ocr BOOLEAN,
  extracted_chars BIGINT DEFAULT 0,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX versions_doc_created_idx ON document_versions(doc_id, created_at DESC);
CREATE INDEX versions_status_idx ON document_versions(status);
Pages
CREATE TABLE document_pages (
  page_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  version_id UUID NOT NULL REFERENCES document_versions(version_id) ON DELETE CASCADE,
  page_num INT NOT NULL,
  page_text TEXT NOT NULL DEFAULT '',
  ocr_used BOOLEAN NOT NULL DEFAULT false,
  ocr_confidence REAL,
  char_count INT GENERATED ALWAYS AS (char_length(page_text)) STORED,
  UNIQUE (version_id, page_num)
);
CREATE INDEX pages_version_page_idx ON document_pages(version_id, page_num);
Chunks
CREATE TABLE chunks (
  chunk_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  version_id UUID NOT NULL REFERENCES document_versions(version_id) ON DELETE CASCADE,
  doc_id UUID NOT NULL REFERENCES documents(doc_id) ON DELETE CASCADE,
  chunk_num INT NOT NULL,
  page_start INT,
  page_end INT,
  char_start INT,
  char_end INT,
  chunk_text TEXT NOT NULL,
  language TEXT NOT NULL DEFAULT 'english', -- detected language for this chunk
  ocr_used BOOLEAN NOT NULL DEFAULT false,
  ocr_confidence REAL,
  fts_en TSVECTOR GENERATED ALWAYS AS (to_tsvector('english', coalesce(chunk_text,''))) STORED,
  fts_fr TSVECTOR GENERATED ALWAYS AS (to_tsvector('french', coalesce(chunk_text,''))) STORED,
  embedding vector(384),
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  UNIQUE (version_id, chunk_num)
);
CREATE INDEX chunks_doc_idx ON chunks(doc_id);
CREATE INDEX chunks_version_idx ON chunks(version_id);
CREATE INDEX chunks_fts_en_idx ON chunks USING GIN(fts_en);
CREATE INDEX chunks_fts_fr_idx ON chunks USING GIN(fts_fr);
CREATE INDEX chunks_embedding_hnsw_idx ON chunks USING hnsw (embedding vector_cosine_ops);
Jobs
CREATE TYPE job_stage AS ENUM ('extract','ocr','chunk','embed','finalize');
CREATE TYPE job_status AS ENUM ('queued','running','done','error');

CREATE TABLE ingestion_jobs (
  job_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  version_id UUID NOT NULL REFERENCES document_versions(version_id) ON DELETE CASCADE,
  stage job_stage NOT NULL,
  status job_status NOT NULL DEFAULT 'queued',
  progress_current INT DEFAULT 0,
  progress_total INT DEFAULT 0,
  metrics JSONB NOT NULL DEFAULT '{}'::jsonb,
  error TEXT,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  started_at TIMESTAMPTZ,
  finished_at TIMESTAMPTZ,
  UNIQUE (version_id, stage)
);
CREATE INDEX jobs_status_idx ON ingestion_jobs(status);
Audit
CREATE TABLE audit_log (
  audit_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(user_id) ON DELETE SET NULL,
  api_key_id UUID REFERENCES api_keys(key_id) ON DELETE SET NULL,
  action TEXT NOT NULL,
  target_type TEXT,
  target_id UUID,
  detail JSONB NOT NULL DEFAULT '{}'::jsonb,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX audit_created_idx ON audit_log(created_at DESC);
J) REST API (explicit endpoints)
Auth
POST /api/auth/login → {access_token, user}
GET /api/me
Admin: /api/users CRUD
Admin: /api/api-keys CRUD
Upload
POST /api/uploads (multipart, multiple files)
GET /api/uploads?since=<iso> (upload + job status)
Docs
GET /api/docs (list doc families)
GET /api/docs/{doc_id} (versions + status)
GET /api/docs/{doc_id}/content?pages=1-3&max_chars=...
Admin: DELETE /api/docs/{doc_id} (soft delete)
Admin: POST /api/docs/{doc_id}/reprocess
Search
POST /api/search
POST /api/passages/read
System
GET /api/system/health
Admin: POST /api/system/purge-run (optional)
Admin: POST /api/system/reaper-run (optional)
SSE (Server-Sent Events)
GET /api/jobs/stream (auth required)
Streams job progress events: {version_id, stage, status, progress, error}
One-directional server→client; uses EventSource on client side.
K) MCP over HTTPS
Endpoint:
POST /mcp (behind gateway HTTPS)
Auth:
User JWT or API key bearer
Tools:
kb_search
kb_read_passages
kb_get_document
kb_list_recent
kb_ingest_status
Admin: kb_reprocess
Admin: kb_system_health
All tools return citations (source filename + page range + excerpt).
L) Logging & verbosity
JSON logs everywhere
Levels: INFO/DEBUG (env var LOG_LEVEL)
Include correlation IDs and version_id/job_id when present
